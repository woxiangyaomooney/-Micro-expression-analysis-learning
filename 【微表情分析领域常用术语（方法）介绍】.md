## 名词/术语

##### **鲁棒性（稳健性）**：

是**在异常和危险情况下系统生存的能力**，是指一个计算机系统在执行过程中处理错误，以及算法在遭遇输入、运算等异常时维持正常运行的能力。

##### 泛化能力：

泛化能力(generalization ability)是指**机器学习算法对新鲜样本的适应能力**。 学习的目的是学到隐含在数据对背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。



##### 光流：

[一文带你入门光流估计](https://aistudio.baidu.com/projectdetail/4597614)

光流法实际是通过检测图像像素点的强度随时间的变化进而推断出物体移动速度及方向的方法。

光流是空间运动物体在成像平面上的像素运动的瞬时速度，是利用图像序列中像素的变化以及相邻帧之间的相关性，来找到上一帧跟当前帧的像素点之间存在的对应关系，从而计算出相邻帧之间像素点的运动信息的一种方法。一般而言，光流是由于场景中前景目标本身的移动、相机的运动，或者两者的共同运动所产生的。

简单说光流描述了第一帧到第二帧所有的像素点移动情况。



##### 纹理特征：

[图像特征提取（纹理特征）](https://sunjackson.github.io/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/2018/07/20/Image_feature_extraction/)

纹理是一种反映图像中同质现象的视觉特征，它体现了物体表面的具有缓慢变化或者周期性变化的表面结构组织排列属性

纹理具有三大标志：

- 某种局部序列性不断重复
- 非随机排列
- 纹理区域内大致为均匀的统一体

不同于灰度、颜色等图像特征，纹理通过像素及其周围空间邻域的灰度分布来表现，即局部纹理信息。另外，局部纹理信息不同程度上的重复性，就是全局纹理信息。

纹理特征体现全局特征的性质的同时，它也描述了图像或图像区域所对应景物的表面性质。但由于纹理只是一种物体表面的特性，并不能完全反映出物体的本质属性，所以仅仅利用纹理特征是无法获得高层次图像内容的。与颜色特征不同，纹理特征不是基于像素点的特征，它需要在包含多个像素点的区域中进行统计计算。在模式匹配中，这种区域性的特征具有较大的优越性，不会由于局部的偏差而无法匹配成功。

![在这里插入图片描述](https://img-blog.csdnimg.cn/822805cdd09543cd91de8e26deb01c50.png)


##### **特征缩放**：

[归一化与标准化](https://ssjcoding.github.io/2019/03/27/normalization-and-standardization/)

特征缩放是用来统一资料中的自变项或特征范围的方法，在资料处理中，通常会被使用在资料前处理这个步骤。因为在原始的资料中，各变数的范围大不相同。

（这周学习的吴恩达机器学习课程刚好讲到这里）

对于大多数的机器学习算法和优化算法来说，将特征值缩放到相同区间可以使得获取性能更好的模型。

例如：

（a）有两个不同的特征，第一个特征的取值范围为1~10，第二个特征的取值范围为1~10000。在梯度下降算法中，代价函数为最小平方误差函数，所以在使用梯度下降算法的时候，算法会明显的偏向于第二个特征，因为它的取值范围更大。

（b）k近邻算法，它使用的是欧式距离，也会导致其偏向于第二个特征。对于决策树和随机森林以及XGboost算法而言，特征缩放对于它们没有什么影响。

常用的特征缩放算法有两种，归一化(normalization)和标准化(standardization)



##### **归一化：**

- 归一化是利用特征的最大值，最小值，将特征的值缩放到[0,1]区间，对于每一列的特征使用min - max函数进行缩放。

- 归一化可以消除纲量，加快收敛。不同特征往往具有不同的量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据归一化处理，以解决数据指标之间的可比性。原始数据经过数据归一化处理后，各指标处于[0,1]之间的小数，适合进行综合对比评价。

-  归一化可能模型提高精度。

例：min－max标准化（Min－max normalization）

- 说明
![在这里插入图片描述](https://img-blog.csdnimg.cn/1ce1d04c7fb141989a9fbc1e2f5b24b9.png)
- 缺点

​	这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。



##### **LBP算子**：

[【Datawhale】计算机视觉下 —— LBP特征描述算子](https://www.cnblogs.com/recoverableTi/p/13195453.html)(内含代码实现)

LBP算子的基本思想是将中心像素的灰度值作为一个阈值，将其邻域内的像素点灰度值进行比较，从而得到二进制编码表示，来表示局部纹理特征。

> 邻域的类型可分为四邻域、D邻域、八邻域，四邻域：该像素点的上下左右四个位置；
>
> D领域：该像素点斜对角线上的四个相邻位置。
>
> 八邻域：四邻域与D邻域的并集。
>
> 基本的LBP算子考虑的是像素的八邻域。

LBP表示方法有一个较为明显的特点，它不容易收到图像整体会读线性变化的影响。也就是说，当图像由于光线的影响使得整体灰度值发生线性均匀变化时，其LBP特征编码是不变的。换句话说，它并不在意整体的灰度变化，而是关注像素之间的相对灰度改变。

例如，在某些情况下，阳光照射强度更低，导致拍摄图像的整体亮度降低，但是实际上每个像素之间的差值仍然是固定的。那么在这种情况下，在图片亮度对LBP特征编码无影响。



## 研究方法：

##### **ASM(Active Shape Model)主动形状模型**：

通过对训练集标记的人脸特征点进行建模（学习），然后在测试集上搜索最佳匹配的点，对人脸特征点进行定位。



##### **级联分类器**：

在一个级联分类系统中，对于每一个输入图片，顺序通过每个强分类器，前面的强分类器相对简单，其包含的弱分类器也相对较少，后面的强分类器逐级复杂，只有通过前面的强分类检测后的图片才能送入后面的强分类器检测，比较靠前的几级分类器可以过滤掉大部分的不合格图片，只有通过了所有强分类器检测的图片区域才是有效人脸区域。

![在这里插入图片描述](https://img-blog.csdnimg.cn/ca6d0c733c834c4481a1a8f105b86e0a.png)


##### **Haar特征与AdaBoost分类器**：

[博文](https://senitco.github.io/2017/06/15/image-feature-haar/)
[知乎](https://zhuanlan.zhihu.com/p/651523263)

现有的haar特征模板主要如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/c1f319f480ba43458356cd9e24a273c5.png)
上图的特征模板称为“特征原型”；特征原型在图像子窗口中扩展（平移、伸缩）得到的特征称为“矩形特征”；矩形特征的值称为“特征值”。从下图可以看到，矩形特征可用于表示人脸的某些特征，如中间一幅表示眼睛区域的颜色比脸颊区域的颜色深，右边一幅表示鼻梁两侧比鼻梁的颜色要深。

<img src="https://img-blog.csdnimg.cn/img_convert/ee169bc5233b64ac088103beab391cfb.png" alt="img" style="zoom:50%;" />


##### **卷积神经网络（CNN）**：

[一文读懂CNN](https://easyai.tech/ai-definition/cnn/)

卷积层运算过程如下：用一个卷积核扫完整张图片

<img src="https://img-blog.csdnimg.cn/img_convert/a474d5287b8d587dc7c392612be83313.gif" alt="卷积层运算过程" style="zoom: 50%;" />

池化层：降维，不但可以大大减少运算量，还可以有效的避免过拟合。

<img src="https://img-blog.csdnimg.cn/img_convert/bcc229082ddf65488a3306cfd1b7cd87.gif" alt="池化层过程" style="zoom: 50%;" />

全连接层：输出结果

<img src="https://img-blog.csdnimg.cn/img_convert/d5dd6a889354fce3e47735682903457f.png" alt="全连接层" style="zoom: 50%;" />

##### **长短期记忆网络(Long Short-Term Memory, LSTM) **：

[长短期记忆网络](https://zh.d2l.ai/chapter_recurrent-modern/lstm.html)

长短期记忆网络——通常被称为 LSTM，是一种特殊的 [RNN](https://easyai.tech/ai-definition/rnn/)，能够学习长期依赖性。由于独特的设计结构，LSTM适合于处理和预测时间序列中间隔和延迟非常长的重要事件。

长短期记忆（LSTM）单位是递归神经网络（RNN）的单位。由LSTM单元组成的RNN通常称为LSTM网络（或仅称为LSTM）。公共LSTM单元由单元，输入门，输出门和忘记门组成。该单元记住任意时间间隔内的值，并且三个门控制进出单元的信息流。

LSTM网络非常适合基于时间序列数据进行分类，处理和预测，因为在时间序列中的重要事件之间可能存在未知持续时间的滞后。开发LSTM是为了处理在训练传统RNN时可能遇到的爆炸和消失的梯度问题。对于间隙长度的相对不敏感性是LSTM相对于RNN，隐马尔可夫模型和其他序列学习方法在许多应用中的优势。



##### **主成分分析：**

[维基百科](https://zh.wikipedia.org/zh-hans/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)        	 [代码实现](https://www.cnblogs.com/lsm-boke/p/11760224.html)

在多元变量分析中，**主成分分析**（英语：**Principal components analysis**，缩写：**PCA**）是一种统计分析、简化数据集的方法。它利用正交变换来对一系列可能相关的变量的观测值进行线性变换，从而投影为一系列线性不相关变量的值，这些不相关变量称为主成分（Principal Components）。具体地，主成分可以看做一个线性方程，其包含一系列线性系数来指示投影方向。PCA对原始数据的正则化或预处理敏感（相对缩放）。

**是一个非监督的机器学习算法**，是一种用于探索高维数据结构的技术，主要用于对数据的降维，通过降维可以发现更便于人理解的特征，加快对样本有价值信息的处理速度，此外还可以应用于可视化（降到二维）和去噪。

**基本思想：**

- 将坐标轴中心移到数据的中心，然后旋转坐标轴，使得数据在C1轴上的方差最大，即全部n个数据个体在该方向上的投影最为分散。意味着更多的信息被保留下来。C1成为**第一主成分**。
- C2**第二主成分**：找一个C2，使得C2与C1的协方差（相关系数）为0，以免与C1信息重叠，并且使数据在该方向的方差尽量最大。
- 以此类推，找到第三主成分，第四主成分……第p个主成分。p个随机变量可以有p个主成分[[1\]](https://zh.wikipedia.org/zh-cn/主成分分析#cite_note-1)。

主成分分析经常用于减少数据集的维数，同时保留数据集当中对方差贡献最大的特征。这是通过保留低维主成分，忽略高维主成分做到的。这样低维成分往往能够保留住数据的最重要部分。但是，这也不是一定的，要视具体应用而定。由于主成分分析依赖所给数据，所以数据的准确性对分析结果影响很大。



##### **主观表观模型（AAM）**：

主动表观模型（AAM）是由Tim Cootes提出的一种参数化表观模型。该模型利用主成分分析对可变性目标的形状和纹理进行统一建模，并使用二范数最小化策略对未知目标进行匹配。主动表观模型被广泛用于目标检测、识别、姿态校正等计算机视觉领域。

建模

AAM的建模过程包括三个部分，形状建模，纹理建模以及统一建模。其中，形状建模首先利用普克拉提斯分析去除尺度、旋转和平移等影响，然后对形状进行主成分分析。纹理建模则在形状建模的基础上将目标间的形状差别消除，单独针对纹理进行主成分分析。而最终的统一建模则是对形状和纹理模型的参数进行主成分分析，从而消除形状和纹理之间的冗余信息。

匹配

AAM将目标的匹配问题看做模型对目标纹理的拟合过程，即通过不断调整模型参数实现模型纹理和目标纹理之间差别的最小化。





